#  DSA2040A â€“ ET Exam (Victor â€“ 388)
*A complete ETL (Extract, Transform, Load) pipeline implemented in Python and SQLite.*

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?logo=python)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Pandas](https://img.shields.io/badge/Pandas-Data%20Analysis-yellow?logo=pandas)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange?logo=jupyter)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen)

---

## ğŸ“˜ Table of Contents
1. [Project Overview](#project-overview)
2. [Repository Structure](#repository-structure)
3. [Dataset Description](#dataset-description)
4. [Extract Phase](#extract-phase)
5. [Transform Phase](#transform-phase)
6. [Helper Module: `data.py`](#helper-module-datapy)
7. [Setup & Usage](#setup--usage)
8. [Requirements](#requirements)
9. [How to Run](#how-to-run)
10. [Expected Outputs](#expected-outputs)
11. [Project Credits & Acknowledgements](#project-credits--acknowledgements)
12. [License](#license)
13. [Contact](#contact)

---

## ğŸš€ Project Overview
This project demonstrates a **complete ETL (Extract, Transform, Load)** pipeline for the course module **DSA2040A â€“ End-Term Exam**.  
It showcases a systematic approach to data processing â€” from raw data collection to validated and transformed datasets ready for analytics.

**Key Highlights:**
- ğŸ“¥ Extraction and validation of raw datasets  
- ğŸ”„ Transformation and enrichment of cleaned data  
- ğŸ’¾ Export of final structured datasets  
- ğŸ§© Modular and reusable Python code design  

**Technologies Used:**
- ğŸ Python (Pandas, NumPy, SQLite3)
- ğŸ““ Jupyter Notebook
- âš™ï¸ Modular scripting with `data.py`

---

## ğŸ“ Repository Structure
```plaintext
DSA2040A_ET_Exam_-Victor-_-388-/
â”‚
â”œâ”€â”€ data/                          # Contains all datasets
â”‚   â”œâ”€â”€ raw/                       # Original unprocessed data
â”‚   â””â”€â”€ validated/                 # Cleaned and validated data
â”‚
â”œâ”€â”€ transformed/                   # Folder for transformed datasets
â”‚   â”œâ”€â”€ transformed_full.csv
â”‚   â””â”€â”€ transformed_incremental.csv
â”‚
â”œâ”€â”€ etl_extract.ipynb              # Data extraction and validation notebook
â”œâ”€â”€ etl_transform.ipynb            # Data transformation notebook
â”œâ”€â”€ data.py                        # Helper module with ETL functions
â””â”€â”€ README.md                      # Project documentation
````

---

## ğŸ“Š Dataset Description

The dataset simulates **real-world data challenges** to demonstrate data cleaning and transformation skills.

### âš™ï¸ Common Issues Addressed

* Missing or null values
* Duplicate and incremental records
* Inconsistent naming conventions
* Mixed data types (numeric/text/date)
* Irregular date and text formatting


---

## ğŸ§© Extract Phase

**Notebook:** `etl_extract.ipynb`

This phase focuses on data **ingestion, inspection, and validation**.

### ğŸ§  Key Steps:

1. Load raw datasets using `data.py` helper functions.
2. Inspect dataset structure and content using `.info()`, `.head()`, `.describe()`.
3. Identify and document data quality issues.
4. Clean and validate the dataset (e.g., remove duplicates, fill nulls, rename columns).
5. Save validated data into `data/validated/`.

### ğŸ’» Example Code

```python
from data import load_data, clean_data, save_data

df_raw = load_data("raw_data.csv")
df_clean = clean_data(df_raw)
save_data(df_clean, "validated_data.csv")
```

ğŸ—‚ï¸ *Output:* A validated, cleaned dataset ready for transformation.

---

## ğŸ” Transform Phase

**Notebook:** `etl_transform.ipynb`

This phase applies multiple **data transformations** across various categories.

### ğŸ§® Transformation Categories

| Category           | Operation Example                              | Goal             |
| :----------------- | :--------------------------------------------- | :--------------- |
| Cleaning           | Remove nulls, trim whitespace                  | Ensure accuracy  |
| Standardization    | Rename columns, fix data types                 | Consistency      |
| Enrichment         | Derive                                         | Add value        |
| Filtering          | Remove irrelevant rows                         | Focused data     |
| Structural Changes | Merge/split columns, change types              | Organized schema |

### ğŸ“¤ Output Files

* `transformed.csv` â€“ Final comprehensive dataset
* `transformed_incremental.csv` â€“ Incremental updates dataset

ğŸ“ *Each transformation is clearly documented in markdown cells with visual outputs.*

---

## âš™ï¸ Helper Module: `data.py`

The `data.py` file contains modular, reusable functions for data handling.

### ğŸ”§ Example Functions

```python
import pandas as pd

def load_data(path):
    """Load dataset into a pandas DataFrame."""
    return pd.read_csv(path)

def clean_data(df):
    """Clean and standardize the dataset."""
    df = df.drop_duplicates()
    df = df.fillna(0)
    df.columns = [col.strip().lower().replace(" ", "_") for col in df.columns]
    return df

def save_data(df, output_path):
    """Save the cleaned/transformed DataFrame to a CSV file."""
    df.to_csv(output_path, index=False)
```

ğŸ§± *This modular approach ensures cleaner, more maintainable code.*

---

## âš™ï¸ Setup & Usage

### ğŸ’¡ Requirements

* Python 3.8+
* Libraries:

  ```bash
  pip install pandas numpy jupyter
  ```

---

## â–¶ï¸ How to Run

1. **Clone the repository**

   ```bash
   git clone https://github.com/VICTOR21-A/DSA2040A_ET_Exam_-Victor-_-388-.git
   cd DSA2040A_ET_Exam_-Victor-_-388-
   ```

2. **Launch Jupyter Notebook**

   ```bash
   jupyter notebook
   ```

3. **Run Extraction Notebook**

   * Open `etl_extract.ipynb`
   * Run all cells
   * Validate the output in `data/validated/`

4. **Run Transformation Notebook**

   * Open `etl_transform.ipynb`
   * Run all cells
   * View results in `transformed/`

---

## ğŸ“ˆ Expected Outputs

| Folder            | File                          | Description                       |
| :---------------- | :---------------------------- | :-------------------------------- |
| `validated/`      | `validated_data.csv`          | Cleaned and validated dataset     |
| `transformed/`    | `transformed.csv`             | Fully transformed dataset         |
| `transformed/`    | `transformed_incremental.csv` | Incrementally transformed dataset |

âœ… *Final datasets are ready for analytics, reporting, or database integration.*

---

## ğŸ™Œ Project Credits & Acknowledgements

* ğŸ‘¨â€ğŸ’» **Developer:** Victor (Student ID: 388)
* ğŸ« **Course:** DSA2040A â€“ End-Term Exam
* ğŸ“š **Institution:** USIU
* ğŸ§‘â€ğŸ« **Instructor:** A. ODERA

**Acknowledgements**

* Open-source Python community
* Libraries: `pandas`, `numpy`, `sqlite3`
* Tools: Jupyter Notebook, VS Code

---

## ğŸ“œ License

This project is licensed under the **MIT License**.
You are free to use, modify, and distribute this code with attribution.
See the [LICENSE](LICENSE) file for full details.

---

## ğŸ“¬ Contact

ğŸ“§ **Victor**
ğŸ”— [GitHub Profile](https://github.com/VICTOR21-A)
ğŸ’¬ *For feedback, collaborations, or inquiries â€” feel free to reach out!*

---



